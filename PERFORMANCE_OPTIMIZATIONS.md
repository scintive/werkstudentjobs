# Performance Optimizations for Job Tailoring

## Current Performance Profile

### âœ… Already Implemented
1. **Database caching** - Existing suggestions returned instantly (line 752)
2. **In-memory caching** - 30-minute TTL for repeated requests (line 778)
3. **Fingerprint-based invalidation** - Only regenerates when job/resume changes
4. **Input trimming** - Reduces token count for GPT calls (line 795)
5. **Model optimization** - Using gpt-4o-mini instead of gpt-4o

### â±ï¸ Performance Breakdown (First Load)
- Variant creation: ~200ms
- Main GPT analysis: **3-5 seconds** âš ï¸ (biggest bottleneck)
- Skills plan fallback: **2-3 seconds** âš ï¸
- Experience bullet top-up: **1-2 seconds** per role
- Database saves: ~500ms
- **Total first load: 7-12 seconds**

### ðŸš€ Recommended Optimizations

## 1. Increase Cache TTL (Quick Win)
**Impact:** Subsequent loads within session are instant
**Change:** Increase from 30 minutes to 24 hours

```typescript
// Line 12
const CACHE_TTL = 24 * 60 * 60 * 1000; // 24 hours instead of 30 minutes
```

**Reasoning:** Job descriptions and resumes rarely change within a day. Longer cache = faster reloads.

---

## 2. Skip Skills Plan Fallback (Medium Win)
**Impact:** Saves 2-3 seconds on first load
**Change:** Make skills plan optional, use simple categorization

The skills plan is nice-to-have but not critical. If the main GPT call doesn't return it, use a simple categorization based on existing categories instead of calling GPT again.

```typescript
// Line 1410 - Replace await with simple fallback
if (!analysisData.skills_category_plan || !Array.isArray(analysisData.skills_category_plan?.categories)) {
  // Simple fallback - use existing skill categories instead of GPT
  analysisData.skills_category_plan = {
    categories: Object.keys(baseResume.skills || {}).map(cat => ({
      canonical_key: cat,
      display_name: cat,
      skills: [],
      priority: 1,
      reasoning: "Preserved from original resume"
    }))
  };
}
```

---

## 3. Reduce Prompt Complexity (Medium Win)
**Impact:** Saves 1-2 seconds per GPT call
**Change:** Simplify the system prompt and reduce examples

Current prompts are very detailed. Shorter prompts = faster responses.

```typescript
// Line 1060 - Simplified prompt example
const systemPrompt = `You are a resume optimizer. Provide concise suggestions to align the resume with the job.

Focus on:
- Professional title and summary alignment
- Experience bullet improvements (2-3 per role)
- Skills matching job requirements

Return JSON with atomic_suggestions array.`;
```

---

## 4. Parallel GPT Calls (High Impact)
**Impact:** Saves 3-5 seconds
**Change:** Run independent GPT calls in parallel

Currently, suggestions are generated sequentially. Run them in parallel:

```typescript
// After line 1208 - Run multiple tasks in parallel
const [analysisData, skillsPlan] = await Promise.all([
  llmService.createJsonResponse({...}),  // Main analysis
  generateSkillPlanFallback({...})        // Skills plan
]);
```

---

## 5. Progressive Loading (Best UX)
**Impact:** User sees results faster
**Change:** Return partial results immediately, enhance in background

```typescript
// After line 1208 - Early return with basic suggestions
if (!force_refresh && existingSuggestions.length > 0) {
  // Return existing suggestions immediately
  return NextResponse.json({
    success: true,
    tailored_resume: variant.tailored_data,
    atomic_suggestions: existingSuggestions,
    loading_enhanced: true  // Signal to frontend
  });

  // Continue generating enhancements in background (don't await)
  enhanceSuggestionsInBackground(variant.id, analysisContext);
}
```

---

## 6. Use Streaming Responses (Advanced)
**Impact:** User sees progress in real-time
**Change:** Stream GPT responses as they arrive

Replace standard completion with streaming:

```typescript
const stream = await openai.chat.completions.create({
  model: 'gpt-4o-mini',
  messages: [...],
  stream: true
});

// Stream back to client
return new Response(stream, {
  headers: { 'Content-Type': 'text/event-stream' }
});
```

---

## 7. Reduce Experience Bullet Top-Up (Quick Win)
**Impact:** Saves 1-2 seconds
**Change:** Skip bullet generation if role already has 3+ bullets

```typescript
// Line 1925 - Add threshold check
const MIN_BULLETS = 3; // Only top-up if fewer than 3
const need = Math.max(0, MIN_BULLETS - currentBullets.length);

if (need <= 0) continue; // Skip if already has enough
```

---

## Priority Implementation Order

### Phase 1 - Quick Wins (15 mins, 30% faster)
1. âœ… Increase cache TTL to 24 hours
2. âœ… Skip skills plan fallback with simple categorization
3. âœ… Reduce experience bullet top-up threshold

### Phase 2 - Medium Impact (1 hour, 50% faster)
4. âœ… Simplify prompt complexity
5. âœ… Parallel GPT calls for independent tasks

### Phase 3 - Advanced (2-4 hours, 70% faster + better UX)
6. âœ… Progressive loading with background enhancement
7. âœ… Streaming responses

---

## Expected Results

### Before Optimizations
- First load: **7-12 seconds**
- Cached load: **instant** (already working)

### After Phase 1
- First load: **5-8 seconds** (30% improvement)
- Cached load: **instant** (no change, already fast)

### After Phase 2
- First load: **3-5 seconds** (50% improvement)
- Cached load: **instant**

### After Phase 3
- Perceived load: **< 1 second** (progressive UI)
- Background completion: **3-5 seconds**
- Full enhancement: **3-5 seconds**

---

## Monitoring Suggestions

Add timing logs to track improvements:

```typescript
const start = Date.now();
console.log(`â±ï¸ GPT call started`);

const result = await llmService.createJsonResponse({...});

console.log(`â±ï¸ GPT call completed in ${Date.now() - start}ms`);
```
