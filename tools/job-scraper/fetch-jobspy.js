/**
 * JobSpy Integration - Fetch jobs and process through GPT pipeline
 *
 * This script:
 * 1. Runs the Python JobSpy scraper to fetch jobs
 * 2. Reads the CSV output
 * 3. Converts to format compatible with import-apify endpoint
 * 4. Processes through existing GPT parsing pipeline
 */

import { exec } from 'child_process';
import { promisify } from 'util';
import fs from 'fs/promises';
import path from 'path';
import { parse } from 'csv-parse/sync';

const execAsync = promisify(exec);

// Configuration
const JOBSPY_SCRIPT = '/Users/varunmishra/Documents/Projects/JobScan/test_jobspy.py';
const JOBSPY_DIR = '/Users/varunmishra/Documents/Projects/JobScan';
const API_ENDPOINT = 'http://localhost:3000/api/jobs/import-jobspy';

async function runJobSpy(resultsWanted = 1) {
  console.log('üöÄ Starting JobSpy scraper...\n');
  console.log(`üìä Requesting ${resultsWanted} job(s) from: LinkedIn, Indeed, Glassdoor, Google`);
  console.log('‚è±Ô∏è  Time filter: Last 72 hours\n');

  try {
    // Create a temporary Python script with modified results_wanted
    // NOTE: Only using Indeed because it provides full job descriptions
    // Glassdoor jobs come without descriptions, making them useless for our GPT pipeline
    const tempScript = `#!/usr/bin/env python3
from jobspy import scrape_jobs
import pandas as pd
from datetime import datetime

print("Scraping ${resultsWanted} werkstudent job(s) from Indeed...")
print("Note: Only using Indeed - it provides full descriptions (Glassdoor doesn't)")

jobs = scrape_jobs(
    site_name=["indeed"],  # Only Indeed - has full descriptions
    search_term="werkstudent",
    location="Germany",
    results_wanted=${resultsWanted},
    hours_old=24,  # Last 24 hours
    country_indeed="germany"
)

print(f"\\nFound {len(jobs)} jobs from Indeed")

if len(jobs) > 0:
    # Save to CSV
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"werkstudent_jobs_{timestamp}.csv"
    jobs.to_csv(output_file, index=False)
    print(f"‚úì Saved to: {output_file}")
    print(f"‚úì Jobs with descriptions: {jobs['description'].notna().sum()}")
else:
    print("‚ùå No jobs found")
`;

    const tempScriptPath = path.join(JOBSPY_DIR, 'temp_scrape.py');
    await fs.writeFile(tempScriptPath, tempScript);

    // Run the Python script with python3.11 (JobSpy requires Python 3.10+)
    const { stdout, stderr } = await execAsync(`cd ${JOBSPY_DIR} && python3.11 temp_scrape.py`, {
      maxBuffer: 10 * 1024 * 1024, // 10MB buffer
    });

    console.log(stdout);
    if (stderr) console.error('‚ö†Ô∏è  Warnings:', stderr);

    // Clean up temp script
    await fs.unlink(tempScriptPath);

    // Find the most recent CSV file
    const files = await fs.readdir(JOBSPY_DIR);
    const csvFiles = files.filter(f => f.startsWith('werkstudent_jobs_') && f.endsWith('.csv'));

    if (csvFiles.length === 0) {
      throw new Error('No CSV output file found');
    }

    // Sort by filename (which includes timestamp) and get the latest
    csvFiles.sort().reverse();
    const latestCsv = csvFiles[0];
    const csvPath = path.join(JOBSPY_DIR, latestCsv);

    console.log(`\nüì• Reading jobs from: ${latestCsv}`);

    // Read and parse CSV
    const csvContent = await fs.readFile(csvPath, 'utf-8');
    const jobs = parse(csvContent, {
      columns: true,
      skip_empty_lines: true,
    });

    console.log(`‚úÖ Successfully parsed ${jobs.length} job(s) from CSV\n`);

    return jobs;

  } catch (error) {
    console.error('‚ùå Error running JobSpy:', error.message);
    throw error;
  }
}

function convertJobSpyToApiFormat(jobspyJob) {
  // Convert JobSpy format to a format compatible with our GPT parser
  return {
    title: jobspyJob.title || '',
    company: jobspyJob.company || '',
    location: jobspyJob.location || '',
    description: jobspyJob.description || '',
    date_posted: jobspyJob.date_posted || '',
    job_url: jobspyJob.job_url || jobspyJob.job_url_direct || '',
    job_type: jobspyJob.job_type || '',
    salary_min: jobspyJob.min_amount || null,
    salary_max: jobspyJob.max_amount || null,
    salary_interval: jobspyJob.interval || null,
    site: jobspyJob.site || '',
    emails: jobspyJob.emails || null,
    company_url: jobspyJob.company_url || jobspyJob.company_url_direct || null,
    company_logo: jobspyJob.company_logo || null,
    company_description: jobspyJob.company_description || null,
  };
}

async function processJobsThroughPipeline(jobs) {
  console.log('üîÑ Processing jobs through GPT parsing pipeline...\n');

  try {
    // Convert jobs to API format
    const formattedJobs = jobs.map(convertJobSpyToApiFormat);

    // Call the import-jobspy endpoint
    const response = await fetch(API_ENDPOINT, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        jobs: formattedJobs,
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Import failed: ${response.status} ${response.statusText}\n${errorText}`);
    }

    const result = await response.json();

    console.log('‚ïê'.repeat(80));
    console.log('‚úÖ PIPELINE COMPLETE');
    console.log('‚ïê'.repeat(80));
    console.log(`üìä Total fetched: ${result.totalFetched || jobs.length}`);
    console.log(`‚úÖ Successfully processed: ${result.processed}`);
    console.log(`‚ùå Failed: ${result.failed}`);

    if (result.jobs && result.jobs.length > 0) {
      console.log('\nüìù Processed Jobs:');
      result.jobs.forEach((job, index) => {
        console.log(`   ${index + 1}. ${job.title} at ${job.company} (ID: ${job.id})`);
      });
    }

    if (result.failedJobs && result.failedJobs.length > 0) {
      console.log('\n‚ùå Failed Jobs:');
      result.failedJobs.forEach((job, index) => {
        console.log(`   ${index + 1}. ${job.title} at ${job.company}`);
        console.log(`      Error: ${job.error}`);
      });
    }

    console.log('\n' + '‚ïê'.repeat(80));
    console.log('üéâ Done! Jobs are now in your database.');
    console.log('‚ïê'.repeat(80) + '\n');

    return result;

  } catch (error) {
    console.error('‚ùå Pipeline error:', error.message);
    throw error;
  }
}

async function main() {
  const resultsWanted = parseInt(process.argv[2]) || 1;

  try {
    console.log('‚ïê'.repeat(80));
    console.log('JOBSPY ‚Üí GPT PIPELINE');
    console.log('‚ïê'.repeat(80) + '\n');

    // Step 1: Run JobSpy scraper
    const jobs = await runJobSpy(resultsWanted);

    if (jobs.length === 0) {
      console.log('‚ö†Ô∏è  No jobs found. Exiting.');
      process.exit(0);
    }

    // Step 2: Process through GPT pipeline
    await processJobsThroughPipeline(jobs);

  } catch (error) {
    console.error('\n‚ùå Error:', error.message);
    if (error.stack) {
      console.error('\nStack trace:');
      console.error(error.stack);
    }
    process.exit(1);
  }
}

// Run the scraper
main();
